{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T06:53:30.441418Z",
     "iopub.status.busy": "2025-01-22T06:53:30.441277Z",
     "iopub.status.idle": "2025-01-22T06:53:41.693023Z",
     "shell.execute_reply": "2025-01-22T06:53:41.691688Z"
    }
   },
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import lightning.pytorch as pl\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "\n",
    "# Paths for datasets\n",
    "train_data_dir = '../../dataset/PBMC_train_set_mapped.h5ad'\n",
    "val_data_dir = '../../dataset/PBMC_val_set_mapped.h5ad'\n",
    "test_data_dir = '../../dataset/PBMC_test_set_mapped.h5ad'\n",
    "\n",
    "# Load the datasets (no change in loading)\n",
    "adata_train = sc.read_h5ad(train_data_dir)\n",
    "adata_val = sc.read_h5ad(val_data_dir)\n",
    "adata_test = sc.read_h5ad(test_data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T06:53:41.695921Z",
     "iopub.status.busy": "2025-01-22T06:53:41.695324Z",
     "iopub.status.idle": "2025-01-22T06:53:48.450839Z",
     "shell.execute_reply": "2025-01-22T06:53:48.450151Z"
    }
   },
   "outputs": [],
   "source": [
    "sc.pp.normalize_total(adata_train, target_sum=1e4)\n",
    "sc.pp.log1p(adata_train)\n",
    "\n",
    "sc.pp.normalize_total(adata_val, target_sum=1e4)\n",
    "sc.pp.log1p(adata_val)\n",
    "\n",
    "sc.pp.normalize_total(adata_test, target_sum=1e4)\n",
    "sc.pp.log1p(adata_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T06:53:48.453040Z",
     "iopub.status.busy": "2025-01-22T06:53:48.452870Z",
     "iopub.status.idle": "2025-01-22T06:53:56.517159Z",
     "shell.execute_reply": "2025-01-22T06:53:56.516488Z"
    }
   },
   "outputs": [],
   "source": [
    "# Access Anndata.X and transform to tensors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Take the union of all unique labels across the three datasets\n",
    "all_labels = np.concatenate([\n",
    "    adata_train.obs['cell_type'].values, \n",
    "    adata_val.obs['cell_type'].values, \n",
    "    adata_test.obs['cell_type'].values\n",
    "])\n",
    "\n",
    "# Step 2: Fit LabelEncoder on the combined labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "# Directly use the 'cell_type' column, assuming it is already encoded as int64\n",
    "X_train = torch.tensor(adata_train.X.toarray()).float().to(device)\n",
    "y_train = torch.tensor(label_encoder.transform(adata_train.obs['cell_type'])).long().to(device)\n",
    "\n",
    "X_val = torch.tensor(adata_val.X).float().to(device)\n",
    "y_val = torch.tensor(label_encoder.transform(adata_val.obs['cell_type'])).long().to(device)\n",
    "\n",
    "X_test = torch.tensor(adata_test.X).float().to(device)\n",
    "y_test = torch.tensor(label_encoder.transform(adata_test.obs['cell_type'])).long().to(device)\n",
    "\n",
    "# The rest of the code remains the same\n",
    "\n",
    "\n",
    "# Create TensorDataset and DataLoader for train, val, test\n",
    "batch_size = 256\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T06:53:56.519212Z",
     "iopub.status.busy": "2025-01-22T06:53:56.519042Z",
     "iopub.status.idle": "2025-01-22T06:53:59.087259Z",
     "shell.execute_reply": "2025-01-22T06:53:59.086593Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanchuangyi/miniconda3/envs/ssl/lib/python3.10/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'\n",
      "  warn(f\"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/home/hanchuangyi/miniconda3/envs/ssl/lib/python3.10/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'\n",
      "  warn(f\"Triton dtype mappings did not load successfully due to an error: {exc.msg}\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Model Initialization\n",
    "ckpt_path = \"../../sc_pretrained/Pretrained Models/BarlowTwins.ckpt\"\n",
    "units_encoder = [512, 512, 256, 256, 64]\n",
    "\n",
    "from self_supervision.models.lightning_modules.cellnet_autoencoder import MLPBarlowTwins\n",
    "from self_supervision.estimator.cellnet import EstimatorAutoEncoder\n",
    "\n",
    "estim = EstimatorAutoEncoder(data_path=None)\n",
    "estim.model = MLPBarlowTwins(\n",
    "    gene_dim=X_train.shape[1],  # Number of genes\n",
    "    batch_size=batch_size,\n",
    "    units_encoder=units_encoder,\n",
    "    CHECKPOINT_PATH=ckpt_path\n",
    ")\n",
    "\n",
    "# Load pre-trained checkpoint\n",
    "checkpoint = torch.load(ckpt_path)\n",
    "estim.model.inner_model.load_state_dict({k.replace('backbone.', ''): v for k, v in checkpoint.items() if 'backbone' in k})\n",
    "\n",
    "# Add classification layer\n",
    "n_classes = len(label_encoder.classes_)\n",
    "estim.model.fc = nn.Linear(units_encoder[-1], n_classes)\n",
    "\n",
    "# Fine-tuning: Enable gradient updates for the inner model\n",
    "for param in list(estim.model.inner_model.parameters()):\n",
    "    param.requires_grad = True\n",
    "\n",
    "estim.model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, estim.model.parameters()), lr=9e-4, weight_decay=0.05)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
    "\n",
    "# Trainer with Lightning\n",
    "estim.trainer = pl.Trainer(accelerator=\"gpu\", devices=1 if torch.cuda.is_available() else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T06:53:59.089731Z",
     "iopub.status.busy": "2025-01-22T06:53:59.089290Z",
     "iopub.status.idle": "2025-01-22T06:54:24.595690Z",
     "shell.execute_reply": "2025-01-22T06:54:24.595130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.4533096645209416, Validation Loss: 3.525856673717499\n",
      "Validation loss improved to 3.525856673717499, resetting patience.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.14050636112884124, Validation Loss: 4.097159638549343\n",
      "No improvement in validation loss. Patience counter: 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.05790462550867943, Validation Loss: 4.6973727309342586\n",
      "No improvement in validation loss. Patience counter: 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.03768684581628961, Validation Loss: 5.223945057753361\n",
      "No improvement in validation loss. Patience counter: 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.03363424789119536, Validation Loss: 5.733627145940607\n",
      "No improvement in validation loss. Patience counter: 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.03360751651550555, Validation Loss: 5.461417147607515\n",
      "No improvement in validation loss. Patience counter: 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.029334898458085408, Validation Loss: 4.402367647850152\n",
      "No improvement in validation loss. Patience counter: 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.019592523601942115, Validation Loss: 5.158466411359383\n",
      "No improvement in validation loss. Patience counter: 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.015802096076467428, Validation Loss: 5.483580448410728\n",
      "No improvement in validation loss. Patience counter: 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.02134154356228482, Validation Loss: 5.360293883265871\n",
      "No improvement in validation loss. Patience counter: 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train Loss: 0.020102212195669936, Validation Loss: 5.321753173163443\n",
      "No improvement in validation loss. Patience counter: 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train Loss: 0.01238552865052102, Validation Loss: 5.418553966464418\n",
      "No improvement in validation loss. Patience counter: 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Train Loss: 0.02152327167673483, Validation Loss: 5.701917883121606\n",
      "No improvement in validation loss. Patience counter: 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Train Loss: 0.016949822525160103, Validation Loss: 5.379720120718985\n",
      "No improvement in validation loss. Patience counter: 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Train Loss: 0.019381837598098396, Validation Loss: 4.504064455176845\n",
      "No improvement in validation loss. Patience counter: 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Train Loss: 0.020323378619757262, Validation Loss: 5.272372249400977\n",
      "No improvement in validation loss. Patience counter: 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Train Loss: 0.021489090608944435, Validation Loss: 6.082648660197402\n",
      "No improvement in validation loss. Patience counter: 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Train Loss: 0.0072614683039369425, Validation Loss: 4.841500827760408\n",
      "No improvement in validation loss. Patience counter: 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Train Loss: 0.012333865210160624, Validation Loss: 4.712962874860475\n",
      "No improvement in validation loss. Patience counter: 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Train Loss: 0.011867061840126281, Validation Loss: 5.475575981718121\n",
      "No improvement in validation loss. Patience counter: 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Train Loss: 0.016169846820160982, Validation Loss: 4.922058528119868\n",
      "No improvement in validation loss. Patience counter: 20/20\n",
      "Early stopping triggered. Stopping training at epoch 21.\n",
      "Loaded best model weights based on validation loss.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training and Validation Pipeline\n",
    "def train_epoch(model, optimizer, loss_fn, train_loader, val_loader):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.inner_model(X_batch)\n",
    "        logits = model.fc(outputs)\n",
    "        loss = loss_fn(logits, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            val_outputs = model.inner_model(X_val_batch)\n",
    "            val_logits = model.fc(val_outputs)\n",
    "            val_loss = loss_fn(val_logits, y_val_batch)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    return total_train_loss / len(train_loader), total_val_loss / len(val_loader)\n",
    "\n",
    "# Early Stopping and Model Saving\n",
    "patience = 20\n",
    "min_delta = 1e-4\n",
    "patience_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "best_model_weights = copy.deepcopy(estim.model.state_dict())\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(500):\n",
    "    train_loss, val_loss = train_epoch(estim.model, optimizer, loss_fn, train_loader, val_loader)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss}, Validation Loss: {val_loss}')\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_weights = copy.deepcopy(estim.model.state_dict())\n",
    "        print(f\"Validation loss improved to {best_val_loss}, resetting patience.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in validation loss. Patience counter: {patience_counter}/{patience}\")\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered. Stopping training at epoch {epoch+1}.\")\n",
    "        break\n",
    "\n",
    "estim.model.load_state_dict(best_model_weights)\n",
    "print(\"Loaded best model weights based on validation loss.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T06:54:24.597330Z",
     "iopub.status.busy": "2025-01-22T06:54:24.597165Z",
     "iopub.status.idle": "2025-01-22T06:54:26.748950Z",
     "shell.execute_reply": "2025-01-22T06:54:26.748357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 0.4208878770631759\n",
      "Weighted F1 Score: 0.3857580637327141\n",
      "Macro F1 Score: 0.2617841079856328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanchuangyi/miniconda3/envs/ssl/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hanchuangyi/miniconda3/envs/ssl/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hanchuangyi/miniconda3/envs/ssl/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hanchuangyi/miniconda3/envs/ssl/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hanchuangyi/miniconda3/envs/ssl/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hanchuangyi/miniconda3/envs/ssl/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      precision    recall  f1-score   support\n",
      "\n",
      "                                              B cell       0.46      0.40      0.43     11338\n",
      "                CD1c-positive myeloid dendritic cell       0.00      0.00      0.00        95\n",
      "                          CD4-positive helper T cell       0.00      0.00      0.00      7784\n",
      "                     CD4-positive, alpha-beta T cell       0.15      0.21      0.18     11008\n",
      "              CD4-positive, alpha-beta memory T cell       0.00      0.00      0.00      2866\n",
      "                     CD8-positive, alpha-beta T cell       0.22      0.22      0.22      8505\n",
      "           CD8-positive, alpha-beta cytotoxic T cell       0.00      0.00      0.00       543\n",
      "              CD8-positive, alpha-beta memory T cell       0.00      0.00      0.00      4408\n",
      "                                        Schwann cell       0.00      0.00      0.00        10\n",
      "                                              T cell       0.17      0.68      0.27      7486\n",
      "                            T follicular helper cell       0.00      0.00      0.00       398\n",
      "                                          basal cell       0.62      0.82      0.70      7466\n",
      "                       blood vessel endothelial cell       0.00      0.00      0.00        47\n",
      "                          capillary endothelial cell       0.53      0.79      0.63      3787\n",
      "                                  classical monocyte       0.51      0.39      0.44      6900\n",
      "                                           club cell       0.19      0.14      0.16      1765\n",
      "                                      dendritic cell       0.10      0.45      0.16        73\n",
      "            effector CD8-positive, alpha-beta T cell       0.00      0.00      0.00      5148\n",
      "                                    endothelial cell       0.43      0.19      0.26      4815\n",
      "                          endothelial cell of artery       0.15      0.20      0.17       885\n",
      "                endothelial cell of lymphatic vessel       0.55      0.61      0.58       684\n",
      "                                          enterocyte       0.00      0.00      0.00      1669\n",
      "                                         erythrocyte       0.99      0.94      0.97      2443\n",
      "                                         goblet cell       0.00      0.00      0.00       192\n",
      "                                         granulocyte       0.00      0.00      0.00       723\n",
      "                             hematopoietic stem cell       0.65      0.56      0.60       466\n",
      "                                innate lymphoid cell       0.00      0.00      0.00      4261\n",
      "                               intermediate monocyte       0.00      0.00      0.00       506\n",
      "                                        keratinocyte       0.00      0.00      0.00      3647\n",
      "                                           leukocyte       0.00      0.00      0.00       497\n",
      "            luminal epithelial cell of mammary gland       0.00      0.00      0.00         0\n",
      "                                          macrophage       0.88      0.91      0.90     17081\n",
      "                                           mast cell       0.91      0.91      0.91       675\n",
      "                                    mature NK T cell       0.60      0.68      0.64      4625\n",
      "                                       memory B cell       0.45      0.59      0.51      9648\n",
      "                                    mesothelial cell       0.78      0.50      0.61        28\n",
      "                                            monocyte       0.28      0.75      0.41      9029\n",
      "                                        naive B cell       0.85      0.72      0.78      3244\n",
      "naive thymus-derived CD4-positive, alpha-beta T cell       0.00      0.00      0.00      2181\n",
      "naive thymus-derived CD8-positive, alpha-beta T cell       0.00      0.00      0.00       469\n",
      "                                          neutrophil       0.00      0.00      0.00     13103\n",
      "                              non-classical monocyte       0.09      0.08      0.09       866\n",
      "                                            pericyte       0.38      0.84      0.52      2989\n",
      "                                         plasma cell       0.95      0.50      0.66      8551\n",
      "                                         plasmablast       0.00      0.33      0.00         3\n",
      "                         plasmacytoid dendritic cell       0.05      0.55      0.09        40\n",
      "                                            platelet       0.00      0.00      0.00        92\n",
      "                                   regulatory T cell       0.00      0.00      0.00       939\n",
      "                                  smooth muscle cell       0.84      0.44      0.58      3340\n",
      "                                   type I pneumocyte       0.03      0.34      0.05        62\n",
      "                                  type II pneumocyte       0.90      0.79      0.84      7717\n",
      "              vascular associated smooth muscle cell       0.32      0.08      0.13      2020\n",
      "                               vein endothelial cell       0.38      0.39      0.39      2639\n",
      "\n",
      "                                            accuracy                           0.42    189756\n",
      "                                           macro avg       0.27      0.30      0.26    189756\n",
      "                                        weighted avg       0.39      0.42      0.39    189756\n",
      "\n",
      "\n",
      "Missing class indices: {39, 14, 18, 52, 53, 55, 25}\n",
      "Missing class names: ['microglial cell', 'cardiac muscle cell', 'double-positive, alpha-beta thymocyte', 'retina horizontal cell', 'retinal ganglion cell', 'tracheal goblet cell', 'fibroblast of cardiac tissue']\n",
      "Saved embeddings, predictions and label mapping to ./prediction_results/barlow_twins_fine_tune_seed_42\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "\n",
    "# Evaluate on Test Set  \n",
    "estim.model.eval()  \n",
    "with torch.no_grad():  \n",
    "    test_embeddings = estim.model.inner_model(X_test).cpu().numpy()  \n",
    "    val_embeddings = estim.model.inner_model(X_val).detach().cpu().numpy()  \n",
    "    train_embeddings = estim.model.inner_model(X_train).detach().cpu().numpy()  \n",
    "\n",
    "# KNN Classification  \n",
    "knn = KNeighborsClassifier(n_neighbors=5)  \n",
    "knn.fit(val_embeddings, y_val.cpu().numpy())  \n",
    "predictions = knn.predict(test_embeddings)  \n",
    "\n",
    "# First, get the actual unique classes present in both y_test and predictions  \n",
    "unique_classes = np.unique(np.concatenate([y_test.cpu().numpy(), predictions]))\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test.cpu().numpy(), predictions)  \n",
    "f1 = f1_score(y_test.cpu().numpy(), predictions, average='weighted')  \n",
    "macro_f1 = f1_score(y_test.cpu().numpy(), predictions, average='macro')  \n",
    "\n",
    "print(f\"KNN Accuracy: {accuracy}\")  \n",
    "print(f\"Weighted F1 Score: {f1}\")  \n",
    "print(f\"Macro F1 Score: {macro_f1}\")  \n",
    "\n",
    "# Get the class names for only the classes present in the data  \n",
    "present_classes = [label_encoder.classes_[i] for i in unique_classes]  \n",
    "report = classification_report(y_test.cpu().numpy(), predictions,   \n",
    "                             labels=unique_classes,  # specify which labels to include  \n",
    "                             target_names=present_classes)  # their corresponding names  \n",
    "print(report)  \n",
    "\n",
    "# Optionally, print which class is missing  \n",
    "all_classes_set = set(range(len(label_encoder.classes_)))  \n",
    "present_classes_set = set(unique_classes)  \n",
    "missing_classes = all_classes_set - present_classes_set  \n",
    "if missing_classes:  \n",
    "    print(\"\\nMissing class indices:\", missing_classes)  \n",
    "    print(\"Missing class names:\", [label_encoder.classes_[i] for i in missing_classes])\n",
    "random_seed = 42\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Create directory to store embeddings and predictions\n",
    "output_dir = os.path.join('./prediction_results', f'barlow_twins_zero_shot_seed_{random_seed}')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save embeddings\n",
    "np.save(os.path.join(output_dir, 'train_embeddings.npy'), train_embeddings)\n",
    "np.save(os.path.join(output_dir, 'val_embeddings.npy'), val_embeddings) \n",
    "np.save(os.path.join(output_dir, 'test_embeddings.npy'), test_embeddings)\n",
    "\n",
    "# Save predictions and ground truth\n",
    "np.save(os.path.join(output_dir, 'test_predictions.npy'), predictions)\n",
    "np.save(os.path.join(output_dir, 'test_ground_truth.npy'), y_test.cpu().numpy())\n",
    "np.save(os.path.join(output_dir, 'train_ground_truth.npy'), y_train.cpu().numpy())\n",
    "np.save(os.path.join(output_dir, 'val_ground_truth.npy'), y_val.cpu().numpy())\n",
    "\n",
    "# Save training history if exists\n",
    "if 'train_losses' in globals() and 'val_losses' in globals():\n",
    "    np.save(os.path.join(output_dir, 'train_losses.npy'), np.array(train_losses))\n",
    "    np.save(os.path.join(output_dir, 'val_losses.npy'), np.array(val_losses))\n",
    "\n",
    "# Save label encoder classes (target names)\n",
    "label_mapping = {i: label_name for i, label_name in enumerate(label_encoder.classes_)}\n",
    "with open(os.path.join(output_dir, 'label_mapping.json'), 'w') as f:\n",
    "    json.dump(label_mapping, f, indent=4)\n",
    "\n",
    "print(f\"Saved embeddings, predictions and label mapping to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T06:54:26.750605Z",
     "iopub.status.busy": "2025-01-22T06:54:26.750444Z",
     "iopub.status.idle": "2025-01-22T06:54:26.758931Z",
     "shell.execute_reply": "2025-01-22T06:54:26.758453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics Summary:\n",
      "init_train_loss\tinit_val_loss\tconverged_epoch\tconverged_val_loss\tmacro_f1\tweighted_f1\tmicro_f1\n",
      "0.453\t3.526\t1\t3.526\t0.262\t0.386\t0.421\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 当前 Notebook 文件名\n",
    "notebook_name = \"PBMC_barlow_twins_fine_tune_42.ipynb\"\n",
    "\n",
    "# 初始化需要打印的值\n",
    "init_train_loss = train_losses[0] if 'train_losses' in globals() else None\n",
    "init_val_loss = val_losses[0] if 'val_losses' in globals() else None\n",
    "converged_epoch = len(train_losses) - patience if 'train_losses' in globals() else None\n",
    "converged_val_loss = best_val_loss if 'best_val_loss' in globals() else None\n",
    "\n",
    "# 打印所有所需的指标\n",
    "print(\"Metrics Summary:\")\n",
    "if 'train_losses' in globals():\n",
    "    print(f\"init_train_loss\\tinit_val_loss\\tconverged_epoch\\tconverged_val_loss\\tmacro_f1\\tweighted_f1\\tmicro_f1\")\n",
    "    print(f\"{init_train_loss:.3f}\\t{init_val_loss:.3f}\\t{converged_epoch}\\t{converged_val_loss:.3f}\\t{macro_f1:.3f}\\t{f1:.3f}\\t{accuracy:.3f}\")\n",
    "else:\n",
    "    print(f\"macro_f1\\tweighted_f1\\tmicro_f1\")\n",
    "    print(f\"{macro_f1:.3f}\\t{f1:.3f}\\t{accuracy:.3f}\")\n",
    "\n",
    "# 保存结果到 CSV 文件\n",
    "output_data = {\n",
    "    'dataset_split_random_seed': [int(random_seed)],\n",
    "    'dataset': ['PBMC'],\n",
    "    'method': [re.search(r'PBMC_(.*?)_\\d+', notebook_name).group(1)],\n",
    "    'init_train_loss': [init_train_loss if init_train_loss is not None else ''],\n",
    "    'init_val_loss': [init_val_loss if init_val_loss is not None else ''],\n",
    "    'converged_epoch': [converged_epoch if converged_epoch is not None else ''],\n",
    "    'converged_val_loss': [converged_val_loss if converged_val_loss is not None else ''],\n",
    "    'macro_f1': [macro_f1],\n",
    "    'weighted_f1': [f1],\n",
    "    'micro_f1': [accuracy]\n",
    "}\n",
    "output_df = pd.DataFrame(output_data)\n",
    "\n",
    "# 保存到当前目录下名为 results 的文件夹中\n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "\n",
    "csv_filename = f\"results/{os.path.splitext(notebook_name)[0]}_results.csv\"\n",
    "output_df.to_csv(csv_filename, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
